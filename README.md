ğŸš€ User Data ETL Pipeline & SQLite Integration
      Assignment #2 â€“ Data Engineering
ğŸ“Œ Project Description & Overview

   In real-world data engineering, data rarely comes in a clean, ready-to-use format. APIs often return nested JSON, missing values, duplicates, or
   inconsistent fields.

   This project simulates a real-world ETL (Extract, Transform, Load) pipeline that extracts user data from an external API, cleans and validates it
   using strict business rules, stores the clean data into an SQLite relational database, and finally generates SQL-based business insights.

   By the end of execution, raw unstructured web data is converted into a structured, analysis-ready business asset.

ğŸ¯ Aim of the Project

   The main objectives of this project are to:

   Extract data from an external API using Python

   Handle unreliable data sources with proper exception handling

   Transform nested JSON data into a flat, tabular structure

   Apply strict validation rules to ensure data quality

   Implement logging to track pipeline execution and failures

   Store validated data into an SQLite relational database

   Generate meaningful business insights using SQL queries

ğŸŒ API Used

   Source API

   https://jsonplaceholder.typicode.com/users


   This API provides nested JSON user profiles, making it ideal for ETL simulation.

ğŸ” Overall ETL Workflow
   API
    â†“
   Extract data (Python)
    â†“
   Transform & clean
    â†“
   Validate data
    â†“
   Save CSVs
    â†“
   Insert into SQLite (Python)
    â†“
   Run SQL insights (Python)

ğŸ§  Step-by-Step Pipeline Flow

1ï¸âƒ£ Extract (Extractor)

   Connects to the external API

   Fetches raw JSON data

   Handles API/network failures gracefully

2ï¸âƒ£ Transform

   Flattens nested JSON fields (address, company)

   Combines address fields into readable format
 
   Prepares clean rows for validation

3ï¸âƒ£ Validate (Quality Gates)

   Each record is validated before storage.
   Invalid records are rejected immediately to protect database integrity.

4ï¸âƒ£ Load

   Saves valid records into:

   CSV files (backup)

   SQLite database (users.db)

   Uses atomic transactions to prevent partial inserts

5ï¸âƒ£ Insights

   Executes SQL aggregation queries

   Produces business insights like:

   Total active users

   Most common city

   Top email domain

ğŸ›¡ï¸ Data Validation Rules
   Rule	Description	Action
   Duplicate user_id	Prevents duplicate users	âŒ Reject
   Email without @	Ensures valid email format	âŒ Reject
   City is null	Mandatory location field	âŒ Reject
   Zipcode length < 5	Ensures valid postal code	âŒ Reject

ğŸ“Œ Rejected records are logged in logs/pipeline.log.

ğŸ—„ï¸ Database Design (SQLite)

   The validated data is stored in an SQLite relational database, enabling efficient querying and analysis.
   <img width="1590" height="459" alt="database" src="https://github.com/user-attachments/assets/a5fb58e3-e4c4-4ede-adff-ba0e7ba8e339" />


ğŸ“‚ Database File
   database/users.db

ğŸ“‹ Table: users
   Column	Description 
   user_id	Primary key
   name	User full name
   email	Validated email
   city	User city
   zipcode	Stored as text
   address	Combined street, suite, city
   phone	Contact number
   company_name	Company name
ğŸ“¸ Actual Database Output (After ETL Run)

   Screenshot below shows the final SQLite database table containing only
   validated user records after successful ETL execution.

âœ” Only validated records are inserted
âœ” Invalid records never pollute the database

âš™ï¸ How the Database Works

   Database is created automatically if not present

   Uses CREATE TABLE IF NOT EXISTS

   Inserts use atomic transactions

   Maintains consistency even if failures occur

   This design mirrors production-grade ETL systems.

ğŸ“ Project File Structure

<img width="1024" height="559" alt="image" src="https://github.com/user-attachments/assets/0c29c554-2ee7-47ef-9a24-fda711994721" />

ğŸ–¥ï¸  Terminal Dashboard
<img width="2816" height="1536" alt="Gemini_Generated_Image_fom695fom695fom6" src="<img width="2816" height="1504" alt="Gemini_Generated_Image_15wl5415wl5415wl" src="https://github.com/user-attachments/assets/2f2693a5-4a88-4b69-888a-4d67d553d10b" />


  Running the pipeline gives real-time feedback for each ETL stage.

python code/main.py

  Sample Output
ğŸš€ Starting ETL Pipeline...
ğŸ“¡ EXTRACT: Fetching data from API...
ğŸ”„ TRANSFORM: Processing 10 raw records...
ğŸ›¡ï¸ VALIDATE: Applying quality rules...
âŒ User 3: Rejected (Zipcode too short)
âœ… 9 Valid records ready for loading.
ğŸ’¾ LOAD: Writing to database/users.db...
âœ… Data successfully saved.

--- ğŸ“Š GENERATED INSIGHTS ---
> Total Active Users: 9
> Most Common City:   Gwenborough
> Top Email Domain:   @april.biz

â–¶ï¸ How to Run the Project
1ï¸âƒ£ Install Dependencies
   pip install -r requirements.txt

2ï¸âƒ£ Run the ETL Pipeline
   python code/main.py

3ï¸âƒ£ Results

   SQLite DB â†’ database/users.db

   Logs â†’ logs/pipeline.log

   CSV backups â†’ data/

âœ… Assignment #2 Requirements Coverage

âœ” Extract data from an external API
âœ” Handle unreliable data sources
âœ” Clean & transform nested JSON
âœ” Apply strict data validation rules
âœ” Implement logging
âœ” Store data into SQLite database
âœ” Generate SQL-based business insights

ğŸ Conclusion

   This project demonstrates a complete ETL lifecycle using Python and SQLite, closely reflecting real-world data engineering pipelines.
   It emphasizes data quality, reliability, and analytical readiness, making it suitable for both academic evaluation and professional portfolios.
